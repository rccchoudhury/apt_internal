{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_tensor shape:  torch.Size([3, 4, 4, 1])\n",
      "B: 3, C: 1, H: 4, W: 4\n",
      "mask 16 shape:  torch.Size([3, 4, 4])\n",
      "mask 32 shape:  torch.Size([3, 2, 2])\n",
      "Cls token indices:  [0, 14, 25]\n",
      "test_tensor shape:  torch.Size([3, 16, 1])\n",
      "Patches 32 shape:  torch.Size([3, 1, 20])\n",
      "Mask32 after adding class token:  torch.Size([3, 20])\n",
      "tensor([[0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "Mask 32 extract: \n",
      "tensor([[1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0]])\n",
      "mask32_extract shape:  torch.Size([3, 20])\n",
      "mask32 extract: \n",
      "tensor([[False, False, False, False, False,  True, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False],\n",
      "        [False, False, False, False, False, False, False, False, False, False,\n",
      "          True, False, False, False, False,  True, False, False, False, False],\n",
      "        [False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False]])\n",
      "tensor([[[420.],\n",
      "         [  0.],\n",
      "         [  1.],\n",
      "         [  3.],\n",
      "         [  4.],\n",
      "         [  5.],\n",
      "         [  6.],\n",
      "         [  7.],\n",
      "         [  8.],\n",
      "         [  9.],\n",
      "         [ 10.],\n",
      "         [ 11.],\n",
      "         [ 12.],\n",
      "         [ 69.],\n",
      "         [420.],\n",
      "         [ 13.],\n",
      "         [ 14.],\n",
      "         [ 15.],\n",
      "         [ 16.],\n",
      "         [ 17.],\n",
      "         [ 18.],\n",
      "         [ 19.],\n",
      "         [ 20.],\n",
      "         [ 69.],\n",
      "         [ 69.],\n",
      "         [420.],\n",
      "         [ 23.],\n",
      "         [ 24.],\n",
      "         [ 25.],\n",
      "         [ 26.],\n",
      "         [ 27.],\n",
      "         [ 28.],\n",
      "         [ 29.],\n",
      "         [ 30.],\n",
      "         [ 31.],\n",
      "         [ 32.],\n",
      "         [ 33.],\n",
      "         [ 34.],\n",
      "         [ 35.],\n",
      "         [ 36.],\n",
      "         [ 37.],\n",
      "         [ 38.]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import einops\n",
    "import itertools\n",
    "\n",
    "test_tensor1 = [\n",
    "    [0, 1, 2, 2],\n",
    "    [3, 4, 2, 2], \n",
    "    [5, 6, 7, 8], \n",
    "    [9, 10, 11, 12]\n",
    "]\n",
    "\n",
    "test_tensor2 = [\n",
    "    [13, 14, 15, 16],\n",
    "    [17, 18, 19, 20],\n",
    "    [21, 21, 22, 22],\n",
    "    [21, 21, 22, 22],\n",
    "]\n",
    "\n",
    "test_tensor3 = [\n",
    "    [23, 24, 25, 26],\n",
    "    [27, 28, 29, 30],\n",
    "    [31, 32, 33, 34],\n",
    "    [35, 36, 37, 38],\n",
    "]\n",
    "\n",
    "mask16 = [\n",
    "    [[1, 1, 0, 0], \n",
    "    [1, 1, 0, 0], \n",
    "    [1, 1, 1, 1],\n",
    "    [1, 1, 1, 1]],\n",
    "    [[1, 1, 1, 1], \n",
    "    [1, 1, 1, 1], \n",
    "    [0, 0, 0, 0],\n",
    "    [0, 0, 0, 0]],\n",
    "    [[1, 1, 1, 1], \n",
    "    [1, 1, 1, 1], \n",
    "    [1, 1, 1, 1],\n",
    "    [1, 1, 1, 1]],\n",
    "]\n",
    "mask32  = [\n",
    "    [[0, 1], [0, 0]],\n",
    "    [[0, 0], [1, 1]],\n",
    "    [[0, 0], [0, 0]],\n",
    "]\n",
    "\n",
    "grp_token = torch.ones(1) * 69\n",
    "cls_token = torch.ones(1) * 420\n",
    "cls_token.unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "test_tensor1 = torch.tensor(test_tensor1).unsqueeze(0).unsqueeze(-1)\n",
    "test_tensor2 = torch.tensor(test_tensor2).unsqueeze(0).unsqueeze(-1)\n",
    "test_tensor3 = torch.tensor(test_tensor3).unsqueeze(0).unsqueeze(-1)\n",
    "mask32 = torch.tensor(mask32)\n",
    "mask16 = torch.tensor(mask16)\n",
    "\n",
    "\n",
    "#test_tensor = torch.cat([test_tensor1, test_tensor2], dim=0)\n",
    "test_tensor = torch.cat([test_tensor1, test_tensor2, test_tensor3], dim=0)\n",
    "print(\"test_tensor shape: \", test_tensor.shape)\n",
    "print(\"B: {}, C: {}, H: {}, W: {}\".format(test_tensor.shape[0], test_tensor.shape[3], test_tensor.shape[1], test_tensor.shape[2]))\n",
    "\n",
    "print(\"mask 16 shape: \", mask16.shape)\n",
    "print(\"mask 32 shape: \", mask32.shape)\n",
    "\n",
    "B = test_tensor.shape[0]\n",
    "num_tokens = []\n",
    "for idx in range(B):\n",
    "    base = mask16[idx].sum().item() + mask32[idx].sum().item()\n",
    "    #if self.cls_token is not None:\n",
    "    base += 1 # handle cls token\n",
    "    #if self.reg_token is not None:\n",
    "    #    base += self.num_reg_tokens\n",
    "    num_tokens.append(int(base))\n",
    "#block_mask = BlockDiagonalMask.from_seqlens(num_tokens)\n",
    "\n",
    "cls_token_indices = [0] + list(itertools.accumulate(num_tokens[:-1]))\n",
    "print(\"Cls token indices: \", cls_token_indices)\n",
    "\n",
    "B, H, W, C = test_tensor.shape\n",
    "test_tensor = test_tensor.reshape(B, H*W, 1)\n",
    "print(\"test_tensor shape: \", test_tensor.shape)\n",
    "patchembed = einops.rearrange(test_tensor, 'b (h w) c -> b c h w', h=4, w=4)\n",
    "patches_32 = einops.rearrange(patchembed, 'b c (h p1) (w p2)-> b c (h w) (p1 p2)', p1=2, p2=2)\n",
    "patches_32 = patches_32.flatten(3)\n",
    "patches_16 = patchembed.flatten(2)\n",
    "\n",
    "expanded_token = grp_token.view(1, -1, 1, 1).expand(\n",
    "    patches_32.shape[0],\n",
    "    -1,\n",
    "    patches_32.shape[2],\n",
    "    1\n",
    ")\n",
    "\n",
    "patches_32 = torch.cat([expanded_token, patches_32], dim=-1)\n",
    "patches_32 = patches_32.flatten(2)\n",
    "print(\"Patches 32 shape: \" , patches_32.shape)\n",
    "patches_to_mask = torch.cat([patches_16, patches_32], dim=2).permute(0, 2, 1)\n",
    "mask16 = mask16.flatten(1)\n",
    "mask32 = mask32.flatten(1)\n",
    "\n",
    "seqlens_32 = torch.ones_like(mask32, device=mask32.device, dtype=torch.int32) * 5\n",
    "seqlens_16 = torch.ones_like(mask16, device=mask16.device, dtype=torch.int32)\n",
    "combined_seqlens = torch.cat([seqlens_16, seqlens_32], dim=1)\n",
    "seq_mask = torch.cat([mask16, mask32], dim=1).bool()\n",
    "all_seqlens = [int(x) for x in combined_seqlens[seq_mask].tolist()]\n",
    "\n",
    "mask32 = mask32.unsqueeze(-1).repeat(1, 1, 4)\n",
    "mask32_extract = mask32 * 0\n",
    "# Add mask for group token; I think this is wrong ??\n",
    "# TODO: Check that it's correct, i just re-ordered.\n",
    "mask32 = torch.cat([mask32[:, :, :1], mask32], dim=-1)\n",
    "mask32 = mask32.flatten(1)\n",
    "print(\"Mask32 after adding class token: \", mask32.shape)\n",
    "print(mask32)\n",
    "\n",
    "class_token_mask = torch.ones_like(mask32_extract[:, :, :1])\n",
    "mask32_extract = torch.cat([class_token_mask, mask32_extract], dim=-1)\n",
    "mask32_extract = mask32_extract.flatten(1)\n",
    "print(\"Mask 32 extract: \")\n",
    "print(mask32_extract)\n",
    "# # Could replace with multiplication...\n",
    "mask32_extract = torch.logical_and(mask32_extract, mask32)\n",
    "print(\"mask32_extract shape: \", mask32_extract.shape)\n",
    "print(\"mask32 extract: \")\n",
    "print(mask32_extract)\n",
    "\n",
    "combined_mask = torch.cat([mask16, mask32], dim=1).bool()\n",
    "combined_extract_mask = torch.cat([mask16, mask32_extract], dim=1).bool()\n",
    "extract_mask = combined_extract_mask[combined_mask]\n",
    "full_patches = patches_to_mask[combined_mask].unsqueeze(0)\n",
    "# PRETENDE WE DO THE ATTENTION HERE>\n",
    "output = full_patches[0][extract_mask]\n",
    "\n",
    "expanded_output = torch.zeros((output.shape[0] + B, output.shape[1]), device=output.device)\n",
    "expanded_output_mask = torch.ones((output.shape[0] + B, 1), dtype=torch.bool, device=output.device)\n",
    "cls_token_indices_tensor = torch.tensor(cls_token_indices, device=expanded_output.device)\n",
    "expanded_output[cls_token_indices] = cls_token.reshape(1, 1)\n",
    "# expanded_output[cls_token_indices] = cls_token.squeeze(0).expand(len(cls_token_indices), -1)\n",
    "expanded_output_mask[cls_token_indices] = False\n",
    "\n",
    "non_cls_indices = torch.where(expanded_output_mask)[0]\n",
    "expanded_output[non_cls_indices] = output\n",
    "expanded_output= expanded_output.unsqueeze(0)\n",
    "\n",
    "print(expanded_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cls token indices:  [0, 14, 25]\n",
      "torch.Size([3, 16, 1])\n",
      "torch.Size([3, 1, 1])\n",
      "[0, 14, 25]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[420],\n",
       "         [  0],\n",
       "         [  1],\n",
       "         [  3],\n",
       "         [  4],\n",
       "         [  5],\n",
       "         [  6],\n",
       "         [  7],\n",
       "         [  8],\n",
       "         [  9],\n",
       "         [ 10],\n",
       "         [ 11],\n",
       "         [ 12],\n",
       "         [ 69],\n",
       "         [420],\n",
       "         [ 13],\n",
       "         [ 14],\n",
       "         [ 15],\n",
       "         [ 16],\n",
       "         [ 17],\n",
       "         [ 18],\n",
       "         [ 19],\n",
       "         [ 20],\n",
       "         [ 69],\n",
       "         [ 69],\n",
       "         [420],\n",
       "         [ 23],\n",
       "         [ 24],\n",
       "         [ 25],\n",
       "         [ 26],\n",
       "         [ 27],\n",
       "         [ 28],\n",
       "         [ 29],\n",
       "         [ 30],\n",
       "         [ 31],\n",
       "         [ 32],\n",
       "         [ 33],\n",
       "         [ 34],\n",
       "         [ 35],\n",
       "         [ 36],\n",
       "         [ 37],\n",
       "         [ 38]]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_tensor1 = [\n",
    "    [0, 1, 2, 2],\n",
    "    [3, 4, 2, 2], \n",
    "    [5, 6, 7, 8], \n",
    "    [9, 10, 11, 12]\n",
    "]\n",
    "\n",
    "test_tensor2 = [\n",
    "    [13, 14, 15, 16],\n",
    "    [17, 18, 19, 20],\n",
    "    [21, 21, 22, 22],\n",
    "    [21, 21, 22, 22],\n",
    "]\n",
    "\n",
    "test_tensor3 = [\n",
    "    [23, 24, 25, 26],\n",
    "    [27, 28, 29, 30],\n",
    "    [31, 32, 33, 34],\n",
    "    [35, 36, 37, 38],\n",
    "]\n",
    "\n",
    "mask16 = [\n",
    "    [[1, 1, 0, 0], \n",
    "    [1, 1, 0, 0], \n",
    "    [1, 1, 1, 1],\n",
    "    [1, 1, 1, 1]],\n",
    "    [[1, 1, 1, 1], \n",
    "    [1, 1, 1, 1], \n",
    "    [0, 0, 0, 0],\n",
    "    [0, 0, 0, 0]],\n",
    "    [[1, 1, 1, 1], \n",
    "    [1, 1, 1, 1], \n",
    "    [1, 1, 1, 1],\n",
    "    [1, 1, 1, 1]],\n",
    "]\n",
    "mask32  = [\n",
    "    [[0, 1], [0, 0]],\n",
    "    [[0, 0], [1, 1]],\n",
    "    [[0, 0], [0, 0]],\n",
    "]\n",
    "\n",
    "grp_token = torch.ones(1) * 69\n",
    "cls_token = torch.ones(1) * 420\n",
    "\n",
    "test_tensor1 = torch.tensor(test_tensor1).unsqueeze(0).unsqueeze(-1)\n",
    "test_tensor2 = torch.tensor(test_tensor2).unsqueeze(0).unsqueeze(-1)\n",
    "test_tensor3 = torch.tensor(test_tensor3).unsqueeze(0).unsqueeze(-1)\n",
    "mask32 = torch.tensor(mask32)\n",
    "mask16 = torch.tensor(mask16)\n",
    "#test_tensor = torch.cat([test_tensor1, test_tensor2], dim=0)\n",
    "test_tensor = torch.cat([test_tensor1, test_tensor2, test_tensor3], dim=0)\n",
    "\n",
    "B = test_tensor.shape[0]\n",
    "num_tokens = []\n",
    "for idx in range(B):\n",
    "    base = mask16[idx].sum().item() + mask32[idx].sum().item()\n",
    "    #if self.cls_token is not None:\n",
    "    base += 1 # handle cls token\n",
    "    #if self.reg_token is not None:\n",
    "    #    base += self.num_reg_tokens\n",
    "    num_tokens.append(int(base))\n",
    "cls_token_indices = [0] + list(itertools.accumulate(num_tokens[:-1]))\n",
    "print(\"Cls token indices: \", cls_token_indices)\n",
    "\n",
    "num_elements = (mask32.sum() + mask16.sum()).int().item()\n",
    "B, H, W, C = test_tensor.shape\n",
    "test_tensor = test_tensor.reshape(B, H*W, 1)\n",
    "print(test_tensor.shape)\n",
    "to_cat = [cls_token.expand(test_tensor.shape[0], 1, 1)]\n",
    "print(to_cat[0].shape)\n",
    "patchembed = torch.cat(to_cat + [test_tensor], dim=1)\n",
    "\n",
    "cls_toks = patchembed[:, 0]\n",
    "patchembed = patchembed[:, 1:]\n",
    "patchembed = einops.rearrange(patchembed, 'b (h w) c -> b c h w', h=4, w=4)\n",
    "\n",
    "mask16 = mask16.flatten(1)\n",
    "num_32_patches = mask32.sum().int()\n",
    "patch32_seqlens = [5 for _ in range(num_32_patches.item())]\n",
    "mask32 = mask32 * 2\n",
    "mask32 = mask32.flatten(1)\n",
    "\n",
    "combined_mask = torch.cat([mask16, mask32], dim=1)\n",
    "clstok_locs = torch.ones_like(mask16[:, :1])*-1\n",
    "combined_mask = torch.cat([clstok_locs, combined_mask], dim=1)\n",
    "\n",
    "valid_patches = combined_mask[combined_mask != 0]\n",
    "grptok_locs = torch.where(valid_patches == 2)\n",
    "mask16_locs = torch.where(valid_patches == 1)\n",
    "\n",
    "# expanded_token = grp_token.view(1, -1, 1, 1).expand(\n",
    "#     patches_32.shape[0],\n",
    "#     1\n",
    "# )\n",
    "\n",
    "expanded_output = torch.zeros((num_elements + B, C), device=test_tensor.device, dtype=test_tensor.dtype)\n",
    "expanded_output[grptok_locs] = 69\n",
    "expanded_output[mask16_locs] = test_tensor[mask16.bool()]\n",
    "expanded_output[cls_token_indices] = cls_toks.long()\n",
    "expanded_output = expanded_output.unsqueeze(0)\n",
    "print(cls_token_indices)\n",
    "expanded_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cls token indices:  [0, 14, 25]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([420.,   0.,   1.,   3.,   4.,   5.,   6.,   7.,   8.,   9.,  10.,  11.,\n",
       "         12.,   2., 420.,  13.,  14.,  15.,  16.,  17.,  18.,  19.,  20.,  21.,\n",
       "         22., 420.,  23.,  24.,  25.,  26.,  27.,  28.,  29.,  30.,  31.,  32.,\n",
       "         33.,  34.,  35.,  36.,  37.,  38.])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "test_tensor1 = [\n",
    "    [0, 1, 2, 2],\n",
    "    [3, 4, 2, 2], \n",
    "    [5, 6, 7, 8], \n",
    "    [9, 10, 11, 12]\n",
    "]\n",
    "\n",
    "test_tensor2 = [\n",
    "    [13, 14, 15, 16],\n",
    "    [17, 18, 19, 20],\n",
    "    [21, 21, 22, 22],\n",
    "    [21, 21, 22, 22],\n",
    "]\n",
    "\n",
    "test_tensor3 = [\n",
    "    [23, 24, 25, 26],\n",
    "    [27, 28, 29, 30],\n",
    "    [31, 32, 33, 34],\n",
    "    [35, 36, 37, 38],\n",
    "]\n",
    "\n",
    "mask16 = [\n",
    "    [[1, 1, 0, 0], \n",
    "    [1, 1, 0, 0], \n",
    "    [1, 1, 1, 1],\n",
    "    [1, 1, 1, 1]],\n",
    "    [[1, 1, 1, 1], \n",
    "    [1, 1, 1, 1], \n",
    "    [0, 0, 0, 0],\n",
    "    [0, 0, 0, 0]],\n",
    "    [[1, 1, 1, 1], \n",
    "    [1, 1, 1, 1], \n",
    "    [1, 1, 1, 1],\n",
    "    [1, 1, 1, 1]],\n",
    "]\n",
    "mask32  = [\n",
    "    [[0, 1], [0, 0]],\n",
    "    [[0, 0], [1, 1]],\n",
    "    [[0, 0], [0, 0]],\n",
    "]\n",
    "\n",
    "grp_token = torch.ones(1) * 69\n",
    "cls_token = torch.ones(1) * 420\n",
    "\n",
    "test_tensor1 = torch.tensor(test_tensor1).unsqueeze(0).unsqueeze(-1)\n",
    "test_tensor2 = torch.tensor(test_tensor2).unsqueeze(0).unsqueeze(-1)\n",
    "test_tensor3 = torch.tensor(test_tensor3).unsqueeze(0).unsqueeze(-1)\n",
    "mask32 = torch.tensor(mask32)\n",
    "mask16 = torch.tensor(mask16)\n",
    "#test_tensor = torch.cat([test_tensor1, test_tensor2], dim=0)\n",
    "test_tensor = torch.cat([test_tensor1, test_tensor2, test_tensor3], dim=0)\n",
    "\n",
    "B = test_tensor.shape[0]\n",
    "num_tokens = []\n",
    "for idx in range(B):\n",
    "    base = mask16[idx].sum().item() + mask32[idx].sum().item()\n",
    "    #if self.cls_token is not None:\n",
    "    base += 1 # handle cls token\n",
    "    #if self.reg_token is not None:\n",
    "    #    base += self.num_reg_tokens\n",
    "    num_tokens.append(int(base))\n",
    "cls_token_indices = [0] + list(itertools.accumulate(num_tokens[:-1]))\n",
    "print(\"Cls token indices: \", cls_token_indices)\n",
    "\n",
    "pos_embeds = torch.ones((1, 17)) * 0.1\n",
    "pos_embeds[:, 0] = -0.1\n",
    "pos_embeds32 = torch.ones(1, 5) * 0.2\n",
    "pos_embeds32[:, 0] = -0.2\n",
    "\n",
    "total_tokens = sum(num_tokens)\n",
    "downsized_img = test_tensor[:, ::2, ::2]\n",
    "\n",
    "patches32 = downsized_img.reshape(3, -1)\n",
    "patches16 = test_tensor.reshape(3, -1)\n",
    "\n",
    "all_patches = torch.cat([patches16, patches32], dim=1)\n",
    "mask16 = mask16.flatten(1)\n",
    "mask32 = mask32.flatten(1)\n",
    "combined_mask = torch.cat([mask16, mask32], dim=1).bool()\n",
    "selected_patches = all_patches[combined_mask]\n",
    "\n",
    "pos_embeds_to_add = torch.cat([pos_embeds[:, 1:], pos_embeds32[:, 1:]], dim=1)\n",
    "pos_embeds_to_add = pos_embeds_to_add.repeat(B, 1)\n",
    "selected_pos_embeds = pos_embeds_to_add[combined_mask]\n",
    "\n",
    "#selected_patches = selected_patches + selected_pos_embeds\n",
    "#ls_token = cls_token + pos_embeds[:, :1]\n",
    "\n",
    "\n",
    "expanded_output = torch.zeros((total_tokens)).float()\n",
    "output_mask = torch.ones(total_tokens)\n",
    "expanded_output[cls_token_indices] = cls_token\n",
    "output_mask[cls_token_indices] = 0\n",
    "expanded_output[output_mask.bool()] = selected_patches.float()\n",
    "expanded_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([419.9000, 419.9000, 419.9000])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expanded_output[cls_token_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, x, entropy_maps):\n",
    "        B = x.shape[0]\n",
    "        # Set this explicitly. Will make formatting a bit easier.\n",
    "        # Only return the patches we will actually use.\n",
    "        with torch.no_grad():\n",
    "            downsized_img = F.interpolate(x, scale_factor=0.5, mode='bilinear', align_corners=False)\n",
    "            mask32, mask16, seqlens, cls_token_indices, total_tokens = self.construct_masks(x, entropy_maps)\n",
    "\n",
    "            patches32 = downsized_img.reshape(B, -1, 3, 16, 16)\n",
    "            patches16 = x.reshape(B, -1, 3, 16, 16)\n",
    "            all_patches = torch.cat([patches16, patches32], dim=1)\n",
    "            mask16 = mask16.flatten(1)\n",
    "            mask32 = mask32.flatten(1)\n",
    "            combined_mask = torch.cat([mask16, mask32], dim=1).bool()\n",
    "            selected_patches = all_patches[combined_mask]\n",
    "\n",
    "        patch_embeds = self.patch_embed16(selected_patches).squeeze(1)\n",
    "        embed_dim = patch_embeds.shape[-1]\n",
    "        # Add pos embed, skip cls token. \n",
    "        pos_embeds_to_add = torch.cat([self.pos_embed[:, 1:], self.pos_embed32[:, 1:]], dim=1)\n",
    "        # repeat along batch dimension.\n",
    "        pos_embeds_to_add = pos_embeds_to_add.repeat(B, 1, 1)\n",
    "        selected_pos_embeds = pos_embeds_to_add[combined_mask]\n",
    "\n",
    "        patch_embeds = patch_embeds + selected_pos_embeds\n",
    "        cls_token = self.cls_token + self.pos_embed[:, :1]\n",
    "        # Add the cls tokens in the right spots, and the embeddings everywhere else.\n",
    "        # Total tokens already handles the class token, no need to add an extra 1.\n",
    "        expanded_output = torch.zeros((total_tokens, embed_dim), device=cls_token.device)\n",
    "        output_mask = torch.ones((total_tokens))\n",
    "        expanded_output[cls_token_indices] = cls_token\n",
    "        output_mask[cls_token_indices] = 0\n",
    "        expanded_output[output_mask.bool()] = patch_embeds\n",
    "        # Add a batch dim = 1.\n",
    "        expanded_output = expanded_output.unsqueeze(0)\n",
    "        \n",
    "        return expanded_output, seqlens, cls_token_indices"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mixedres2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
