# @package _global_

# to execute this validation run:
# python src/train.py experiment=train_vit_transformer

defaults:
  - /vit_variant@model: flexivit_small
  - override /data: imagenet
  - override /model: vit
  - override /trainer: default
  - override /callbacks: eval
  - override /logger: wandb
  #- _self_

tags: ["imagenet", "vit_ours", "train"]

seed: 42

# Disable training and testing
train: true
test: true

trainer:
  accelerator: "gpu"
  devices: [0]
  precision: "16-mixed"
  enable_checkpointing: false
  max_epochs: 50
  limit_train_batches: 1.0
  limit_val_batches: 1.0
  logger: false

model:
  checkpoint_path: "./checkpoints/vit_large_patch16_224_augreg.pth"
  net:
    _target_: src.models.vision_transformer.VisionTransformer 
    img_size: 96
    mixed_patch_embed:
      _target_: src.models.patch_embed.MixedSquarePatchEmbed
      _partial_: true
    patch_size: 16  # Smallest patch size
    num_scales: 2       # Will use [16, 32]
    thresholds: [6.0]  # Thresholds between scales, len = num_scales - 1
    global_pool: 'token' # use this for some models
    patch_drop_rate: 0.
    no_embed_class: false
    weight_init: 'skip'
  compile: false
  match_head_shape: false
  scheduler_cfg: 
    _target_: src.models.optim_utils.CosineSchedulerConfig
    # Partial bc batch size depends on other stuff.
    warmup_epochs: 5
    total_epochs: 50
    lr: 5e-4
    end_lr: 1e-6
    start_lr: 1e-6
  optimizer: 
    lr: 1
    _target_: torch.optim.AdamW
    _partial_: true
    betas: [0.9, 0.999]
    weight_decay: 0.05

data:
  img_size: 96
  batch_size: 256
  num_workers: 12
  pin_memory: true
